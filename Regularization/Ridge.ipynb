{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regualriztion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression** or **L2 Regularization** or **Tikhonov Regualrization**\n",
    "\n",
    "It is a technique used in machine learning to address the issue of multicollinearity(where predictor variables are highly correlated) and overfitting issues in the model.\n",
    "\n",
    "Ridge regressiona adds a penalty term to the ordinary least squares loss function. This penalty is proportional to the sum of the ocefficient of the squared values.\n",
    "\n",
    "**How Does It Work?**\n",
    "\n",
    "Regular Linear Regression tries to find the best fit line by minimizing the error between predicted and actual values.\n",
    "\n",
    "Ridge Regression does the same thing but adds a small \"penalty\" for large coefficients. This penalty makes the coefficients smaller, so the model becomes simpler and more robust.\n",
    " \n",
    " **When to use?**\n",
    "\n",
    " - When predictor variabble are highly correlated\n",
    " - When the number of predictors exceeds the number of observations\n",
    " - To prevent the overfitting in linear regression models\n",
    "\n",
    " **Advantages**\n",
    "\n",
    " - It mitigates the effect of collinearity\n",
    " - Retains all the predictors in the model, unlike Lasso, which may exclude some predictors by setting their coefficient to zero\n",
    " - Stabilizes model predictions\n",
    "\n",
    " **Disadvantages**\n",
    " \n",
    " - The interpretation of coefficients become less straightforward due to shrinkage\n",
    " - Requires tuning of the hyperparmeter, often through cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
